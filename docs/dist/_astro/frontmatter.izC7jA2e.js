const e="reference/frontmatter.mdx",n="docs",t="reference/frontmatter",o='\nThe frontmatter in Prompt files contains metadata and configuration for the prompt. It is defined using YAML syntax at the beginning of the file, enclosed between triple dashes (`---`).\n\n## Structure\n\n```handlebars\n---\n# Frontmatter content goes here\n---\n\nPrompt template goes here.\n```\n\nAll frontmatter fields are considered optional in a given `.prompt` file, as Prompt implementations may provide default values for any or all of them. If no frontmatter fields need to be specified in a given file, the frontmatter may be omitted entirely and only a prompt template provided.\n\n## Full Example\n\nHere\'s an example of a Prompt frontmatter:\n\n```handlebars\n---\nname: greetingPrompt\nvariant: formal\nmodel: googleai/gemini-1.5-flash\ntools: \n  - timeOfDay\nconfig:\n  version: gemini-1.5-flash-latest\n  temperature: 0.7\n  topK: 20\n  topP 0.8\n  stopSequences:\n    - STOPSTOPSTOP\ninput:\n  default:\n    name: Guest\n  schema:\n    name: string\n    language?: string\noutput:\n  format: json\n  schema:\n    greeting: string, the greeting to provide to the guest\n    formalityLevel: number, the level of formality of your response\nmetadata:\n  customKey:\n    customValue: 123\n---\n```\n\n## Available Fields\n\n### `name`\n- **Type:** `string`\n- **Description:** The name of the prompt. If unspecified, inferred by the filename of the loaded prompt (e.g. `myPrompt.prompt` has an inferred name of `myPrompt`).\n\n### `variant`\n- **Type:** `string`\n- **Description:** The variant name for the prompt. If unspecified, inferred by the filename of the loaded prompt (e.g. `myPrompt.variant1.prompt` has inferred name of `myPrompt` and inferred variant of `variant1`).\n\n### `model`\n- **Type:** `string` or `ModelArgument<Options>`\n- **Description:** The name of the model to use for this prompt, e.g., `googleai/gemini-1.5-flash-latest`. The Prompt implementation is responsible for resolving a string model name to an executable model.\n\n### `tools`\n- **Type:** `string[]`\n- **Description:** Names of tools (registered in the Prompt implementation) to allow use of in this prompt.\n\n### `config`\n- **Type:** `object`\n- **Description:** Configuration to be passed to the model implementation as a `Map<string,any>`. The specific configuration options vary depending on the model implementation.\n- **Common Config:** Model implementations should respect the following config properties where applicable:\n  - `version`:\n    - **Type:** `string`\n    - **Description:** While `model` specifies a model "family" (e.g. Gemini 1.5 Flash, Claude 3.5 Sonnet), `version` specifies a particular version/checkpoint of the model to use. This allows for version pinning and reproducibility of results.\n    - **Example:**  `"gemini-1.5-pro-0801"` or `"v2"`\n\n  - `temperature`:\n    - **Type:** `number`\n    - **Description:** Controls the randomness of the model\'s output. Higher values (e.g., `0.8`) make the output more random, while lower values (e.g., `0.2`) make it more deterministic.\n    - **Range:** Typically between 0 and 1, depends on model implementation.\n    - **Example:**  `0.7`\n\n  - `maxOutputTokens`:\n    - **Type:** `number`\n    - **Description:** Limits the maximum number of tokens in the model\'s response. This can help control the length of the generated text.\n    - **Example:**  `150`\n\n  - `topK`:\n    - **Type:** `number`\n    - **Description:** Limits the number of highest probability vocabulary tokens to consider at each step. This can help focus the model\'s output on more likely tokens.\n    - **Example:**  `40`\n\n  - `topP`:\n    - **Type:** `number`\n    - **Description:** Sets a probability threshold for token selection. The model will only consider tokens whose cumulative probability exceeds this threshold.\n    - **Range:** Typically between 0 and 1\n    - **Example:**  `0.95`\n\n  - `stopSequences`:\n    - **Type:** `string[]`\n    - **Description:** An array of strings that, if encountered, will cause the model to stop generating further output. This is useful for controlling where the model\'s response should end.\n    - **Example:**  `["END", "\\n\\n", "User:"]`\n\n### `input`\n- **Type:** `object`\n- **Description:** Defines the input variables the prompt. If `input` is not specified, the implementation should accept any `Map<string,any>` values as input and pass them to the prompt template.\n- **Properties:**\n  - `default`:\n    - **Type:** `any`\n    - **Description:** Defines the default input variable values to use if none are provided. Input values passed from the implementation should be merged into these values with a shallow merge strategy.\n  - `schema`:\n    - **Type:** [Schema](schema)\n    - **Description:** Schema representing the input values for the prompt. Must correspond to a JSON Schema `object` type.\n\n### `output`\n- **Type:** `object`\n- **Description:** Defines the expected model output format.\n- **Properties:**\n  - `format`:\n    - **Type:** `string`\n    - **Common Values:** `json`, `text`\n    - **Description:** Desired output format for this prompt. Output formats are implementation-specific, but \n  - `schema`:\n    - **Type:** [Schema](schema)\n    - **Description:** Schema representing the expected output from the prompt. Must correspond to a JSON Schema `object` type.\n\n### `metadata`\n- **Type:** `Map<string, any>`\n- **Description:** Arbitrary metadata to be used by code, tools, and libraries.',i={title:"Prompt Frontmatter",author:"Firebase",sort_order:1,draft:!1,tags:[],hide_breadcrumbs:!1,hide_toc:!1,hide_sidenav:!1,max_width:!1},r={type:"content",filePath:"/workspaces/.prompt/docs/src/content/docs/reference/frontmatter.mdx",rawData:void 0};export{r as _internal,o as body,n as collection,i as data,e as id,t as slug};
